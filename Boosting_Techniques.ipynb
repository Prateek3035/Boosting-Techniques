{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#THEORY ANSWERS\n",
        "\n",
        "1. What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "   - Boosting in machine learning is an **ensemble technique** that combines multiple weak learners, typically models performing slightly better than random guessing, to create a strong learner with high accuracy. It works by training weak learners sequentially, where each new learner focuses more on the samples misclassified by previous models by adjusting their weights. Initially, all data points have equal weights, but after each iteration, weights of incorrectly classified samples increase so the next learner prioritizes them. The predictions of all learners are then combined using weighted voting (classification) or weighted averaging (regression). This process significantly improves weak learners by reducing bias and leveraging their strengths, transforming them into a robust model. Popular algorithms implementing boosting include AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost.\n",
        "\n",
        "2. What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "   - The main difference between AdaBoost and Gradient Boosting lies in how they train models and handle errors.\n",
        "\n",
        "* AdaBoost (Adaptive Boosting) trains weak learners sequentially by adjusting the weights of training samples. Misclassified samples get higher weights so that the next weak learner focuses more on those difficult cases. The final model combines learners based on their weighted votes.\n",
        "\n",
        "* Gradient Boosting, on the other hand, trains models by optimizing a loss function using gradient descent. Instead of reweighting samples, it fits each new weak learner to the residual errors (negative gradients) of the previous model, gradually reducing the overall loss.\n",
        "\n",
        "In short, AdaBoost emphasizes sample weighting, while Gradient Boosting emphasizes minimizing loss through gradient-based optimization.\n",
        "\n",
        "3. How does regularization help in XGBoost?\n",
        "   - Regularization in XGBoost helps prevent overfitting and improves the generalization ability of the model by penalizing model complexity. XGBoost includes two types of regularization:\n",
        "\n",
        "L1 Regularization (Lasso) – Encourages sparsity in the model by shrinking less important feature weights toward zero, effectively performing feature selection.\n",
        "L2 Regularization (Ridge) – Penalizes large weights to keep the model stable and prevent over-reliance on specific features.\n",
        "\n",
        "These penalties are applied to the leaf weights of the decision trees in the objective function, which controls tree complexity and prevents the model from becoming too deep or overly specific to the training data. This leads to more robust and generalizable predictions.\n",
        "\n",
        "4. Why is CatBoost considered efficient for handling categorical data?\n",
        "   - CatBoost is considered efficient for handling categorical data because it uses a specialized encoding technique called target-based encoding with ordered boosting, which avoids data leakage and overfitting. Instead of traditional one-hot encoding (which increases dimensionality), CatBoost converts categorical features into numerical representations based on target statistics (like mean target value) in a way that respects the training order, ensuring unbiased estimates. Additionally, CatBoost automatically handles categorical features internally, reducing preprocessing effort and preserving important category relationships. This approach, combined with efficient GPU support and optimized algorithms, makes CatBoost highly effective for datasets with many categorical variables.\n",
        "\n",
        "5.\n",
        "   - Boosting techniques are preferred over bagging methods in scenarios where reducing bias and achieving high predictive accuracy are critical. Some real-world applications include:\n",
        "\n",
        "* Credit Scoring & Fraud Detection – Boosting models like XGBoost and LightGBM handle imbalanced data well and capture complex patterns for detecting fraudulent transactions.\n",
        "* Search Engine Ranking – Gradient Boosting is widely used in ranking algorithms to improve relevance in search results (e.g., Google, Bing).\n",
        "* Recommendation Systems – Boosting models enhance personalization by accurately predicting user preferences.\n",
        "* Healthcare Diagnostics – Used for disease prediction and risk assessment where high accuracy is essential (e.g., cancer detection).\n",
        "* Customer Churn Prediction – Boosting helps identify subtle patterns in customer behavior leading to churn.\n",
        "* Financial Market Prediction – Applied in stock price forecasting and risk modeling for improved decision-making.\n",
        "\n",
        "These applications favor boosting because it reduces bias and improves accuracy compared to bagging methods like Random Forest, which mainly focus on reducing variance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r1lAwmmhctkE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-XQ-5SWcok8",
        "outputId": "6024668f-0489-4af9-9b30-46bc6806958c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9649122807017544\n"
          ]
        }
      ],
      "source": [
        "# Write a Python program to:\n",
        "# ● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "# ● Print the model accuracy\n",
        "\n",
        "# Python Program to train an AdaBoost Classifier on Breast Cancer dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train AdaBoost classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "# ● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "# ● Evaluate performance using R-squared score\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared Score:\", r2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZAsrIiefiTN",
        "outputId": "0ae8d2f2-0e15-4084-fbe4-1864bd577323"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "# ● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "# ● Tune the learning rate using GridSearchCV\n",
        "# ● Print the best parameters and accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define parameter grid for learning rate\n",
        "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHURTbJ1f__H",
        "outputId": "8941e292-81db-4808-9b5e-0d16db61a14a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Test Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [11:20:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "# ● Train a CatBoost Classifier\n",
        "# ● Plot the confusion matrix using seaborn\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=data.target_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yk-DfjdUhN7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "\n",
        "Data Preprocessing\n",
        "\n",
        "* Handle missing values: keep `NaN` (CatBoost/XGBoost handle natively) + missing flags.\n",
        "* Categorical features: use CatBoost (native support) or target encoding for XGBoost.\n",
        "* No scaling needed for trees.\n",
        "* Handle imbalance using class weights or `scale_pos_weight`.\n",
        "\n",
        "Choice of Algorithm\n",
        "\n",
        "* **CatBoost** preferred for mixed numeric/categorical features (handles both well, reduces preprocessing).\n",
        "* XGBoost for numeric-heavy datasets.\n",
        "* AdaBoost only as a baseline.\n",
        "\n",
        "Hyperparameter Tuning\n",
        "\n",
        "* Use Randomized Search → Bayesian/Optuna.\n",
        "* Key params: `learning_rate`, `n_estimators`, `depth`, `subsample`, L1/L2 regularization.\n",
        "* Apply early stopping.\n",
        "\n",
        "Evaluation Metrics\n",
        "\n",
        "* Primary: PR-AUC (best for imbalance).\n",
        "* Secondary: ROC-AUC, Precision\\@K, Recall\\@K, F1-score.\n",
        "* Use cost-sensitive thresholding for business goals.\n",
        "\n",
        "Business Benefits\n",
        "\n",
        "* Reduce loan defaults → lower credit losses.\n",
        "* Enable risk-based pricing and better approval decisions.\n",
        "* Support compliance with explainable predictions (SHAP).\n",
        "* Improve collections efficiency and capital optimization.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9z0Qjn1vhq6T"
      }
    }
  ]
}